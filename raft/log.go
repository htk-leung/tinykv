// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"fmt"
	"github.com/pingcap-incubator/tinykv/log"
	pb "github.com/pingcap-incubator/tinykv/proto/pkg/eraftpb"
)

// RaftLog manage the log entries, its struct look like:
//
//	snapshot/first.....applied....committed....stabled.....last
//	--------|------------------------------------------------|
//	                          log entries
//
// for simplify the RaftLog implement should manage all log entries
// that not truncated
type RaftLog struct {
	// storage contains all stable entries since the last snapshot.
	storage Storage

	// committed is the highest log position that is known to be in
	// stable storage on a quorum of nodes.
	committed uint64

	// applied is the highest log position that the application has
	// been instructed to apply to its state machine.
	// Invariant: applied <= committed
	applied uint64

	// log entries with index <= stabled are persisted to storage.
	// It is used to record the logs that are not persisted by storage yet.
	// Everytime handling `Ready`, the unstabled logs will be included.
	stabled uint64

	// all entries that have not yet compact.
	entries []pb.Entry

	// the incoming unstable snapshot, if any.
	// (Used in 2C)
	pendingSnapshot *pb.Snapshot

	// Your Data Here (2A).

}

/*
	Log replication
	To implement log replication, you may want to start with

	- handling MsgAppend and MsgAppendResponse on both the sender and receiver sides.
	- Checkout raft.RaftLog in raft/log.go which is a helper struct that helps you manage the raft log,
	  in here you also need to interact with the upper application by the Storage interface defined in raft/storage.go
	to get the persisted data like log entries and snapshot.

	You can run make project2ab to test the implementation and see some hints at the end of this part.
*/

// newLog returns log using the given storage. It recovers the log
// to the state that it just commits and applies the latest snapshot.
func newLog(storage Storage) *RaftLog {
	// Your Code Here (2A).

	// note stabled, committed and applied DON'T start counting from 0
	// they reflect actual index value in log
	// ref type MemoryStorage struct : ents[i] has raft log position i+snapshot.Metadata.Index

	hardstate, _, err := storage.InitialState()
	if err != nil {
		panic(err.Error())
	}

	entriesStartInd, err := storage.FirstIndex()
	if err != nil {
		panic(err.Error())
	}

	entriesLastInd, err := storage.LastIndex()
	if err != nil {
		panic(err.Error())
	}
	// entries never returns the dummy entry
	// entriesLastInd + 1 because ents[hi] is excluded from slice being returned
	entries, err := storage.Entries(entriesStartInd, entriesLastInd + 1) 
	if err != nil {
		panic(err.Error())
	}
	if len(entries) == 0 {
		entries = []pb.Entry{{Term: 0, Index: 0}}
	}

	return &RaftLog{
		storage: 			storage, 			// storage contains all stable entries since the last snapshot.
												// refer to section "To restart a node from previous state:" in raft/doc.go
		stabled: 			entriesLastInd, 	// index of last entry that exists in storage
		committed: 			hardstate.Commit, 	// index of last entry replicated to a quorum of peers
		applied: 			0, 					// index of last entry applied locally, not available here, but available in config, needs to be applied when calling newRaft
		entries:			entries, 			// all entries retrieved from storage
		pendingSnapshot: 	nil,				// (Used in 2C)
	}
}

// We need to compact the log entries in some point of time like
// storage compact stabled log entries prevent the log entries
// grow unlimitedly in memory
func (l *RaftLog) maybeCompact() {
	// Your Code Here (2C).
}

// allEntries return all the entries not compacted.
// note, exclude any dummy entries from the return value.
// note, this is one of the test stub functions you need to implement.
func (l *RaftLog) allEntries() []pb.Entry {
	// Your Code Here (2A).
	
	// create empty slice
	ret := make([]pb.Entry, 0, len(l.entries))

	// append to slice if data not empty
	for _, entry := range l.entries {
		if entry.Data != nil {
			ret = append(ret, entry)
		}
	}
	return ret
}

// unstableEntries return all the unstable entries
func (l *RaftLog) unstableEntries() []pb.Entry {
	// Your Code Here (2A).

	ret := l.entries[l.stabled + 1 : ]
	return ret
}

// nextEnts returns all the committed but not applied entries
func (l *RaftLog) nextEnts() (ents []pb.Entry) {
	// Your Code Here (2A).

	ret := l.entries[l.applied + 1 : l.committed + 1]
	return ret
}

// LastIndex return the last index of the log entries
// INCLUDING UNSTABLE ONES
func (l *RaftLog) LastIndex() uint64 {
	// Your Code Here (2A).

	firstind, err := l.storage.FirstIndex() // (uint64, error)
	if err != nil {
		log.Panic("LastIndex(): FirstIndex() not available from RaftLog.storage")
	}

	entlen := len(l.entries)
	
	return firstind + uint64(entlen) - 1
}

// Term return the term of the entry in the given index
func (l *RaftLog) Term(i uint64) (uint64, error) {
	// Your Code Here (2A).

	lastind := l.LastIndex()
	firstind, err := l.storage.FirstIndex()
	if err != nil {
		return 0, err
	}

	if i >= firstind && i <= lastind {
		return l.entries[i].Term, nil
	}

	return 0, fmt.Errorf("Term(i): Index(%d) out of bounds of entries currently available in RaftLog", i)
}